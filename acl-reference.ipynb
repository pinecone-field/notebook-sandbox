{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-field/notebook-sandbox/blob/main/acl-reference.ipynb)\n",
    "\n",
    "This notebook provides a reference for how to create Access Control List(ACL) rules for Retrieval Augmented Generation(RAG) based vector search.\n",
    "\n",
    "![alt text](acl_rules.png)\n",
    "\n",
    "# Step #1 - Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"pinecone-client[grpc]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step #2 - Create Pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "API_KEY = getpass.getpass(\"Enter your API key: \")\n",
    "ENVIRONMENT = getpass.getpass(\"Enter your environment: \")\n",
    "INDEX_NAME = \"acl-quicktest\"\n",
    "DIMENSNIONS = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "pinecone.init(api_key=API_KEY, environment=ENVIRONMENT)\n",
    "\n",
    "if (INDEX_NAME in pinecone.list_indexes()) != True:  \n",
    "    pinecone.create_index(INDEX_NAME, dimension=512, metric=\"cosine\", pods=1, replicas=1, pod_type=\"s1.x1\")\n",
    "else:\n",
    "    print(f\"Index {INDEX_NAME} already exists\")\n",
    "\n",
    "index = pinecone.Index(INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step #3 - Generate 5000 vectors\n",
    "\n",
    "### Namespace ACL filter \n",
    "1. Randomly pick a workspace_id: \"1\", \"2\", \"3\"\n",
    "\n",
    "### Meta-data ACL filter \n",
    "2. Randomly pick a group_id: \"1\", \"2\", \"3\"\n",
    "3. Randomly pick a doc_type_id: \"1\", \"2\", \"3\"\n",
    "4. Generate a dummy vector embedding using a value seed that gets incremented for each additional vector\n",
    "\n",
    "### Set vector ID to doc_id with child chunk_id composite key\n",
    "5. Naming convention for vector id is: \"document-{doc_id}-{chunk_count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import uuid\n",
    "\n",
    "def generate_vectors():\n",
    "    float_seed = 0.1\n",
    "    doc_id = 1\n",
    "    for _ in range(100):\n",
    "        vectors = []\n",
    "        meta_data = {\"group_id\": random.choice([\"1\", \"2\", \"3\"]),\n",
    "                     \"doc_type\" : random.choice([\"1\", \"2\", \"3\"])}\n",
    "        \n",
    "        id = uuid.uuid4()\n",
    "        chunk_count = 0\n",
    "        for _ in range(50):             \n",
    "            embeddings = np.full(shape=DIMENSNIONS, fill_value=float_seed).tolist()\n",
    "            vectors.append({'id': f\"document-{doc_id}-{chunk_count}\",\n",
    "                            'values': embeddings,\n",
    "                            'metadata': meta_data})\n",
    "            chunk_count = chunk_count + 1\n",
    "            float_seed = float_seed + 0.1\n",
    "        \n",
    "        workspace = random.choice([\"1\", \"2\", \"3\"])\n",
    "        index.upsert(vectors, namespace=workspace)\n",
    "        index.update(f\"document-{doc_id}-0\", set_metadata={\"chunk_count\": chunk_count}, namespace=workspace)\n",
    "        doc_id = doc_id + 1\n",
    "\n",
    "generate_vectors()\n",
    "index.describe_index_stats()\n",
    "print(f\"Index Stats: {index.describe_index_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step #4 - Test ACL query logic\n",
    "\n",
    "1. Set `workspace` to the workspace id to limit query results to a specific namespace\n",
    "1. Alter `group_ids` list to see if group_id filter is being applied correctly\n",
    "1. Alter `doc_type_ids` list to see if doc_type filter is being applied correctly\n",
    "\n",
    "You should see that query results are limited to just the workspace, group_ids and subgroup_ids specified in the \n",
    "`acl_query()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acl_query(group_ids, doctype_ids, workspace, top_k, vector):\n",
    "    query_results = index.query(vector=vector, top_k=top_k, namespace=workspace, include_metadata=True,\n",
    "                                filter={ \"$and\": [{ \"group_id\": { \"$in\": group_ids } }, \n",
    "                                                  { \"doc_type\": { \"$in\": doctype_ids } }]}).matches\n",
    "    return query_results\n",
    "\n",
    "workspace = \"3\"\n",
    "group_ids = [\"1\",\"2\",\"3\"]\n",
    "doctype_ids = [\"1\",\"2\",\"3\"]   \n",
    "\n",
    "vector = np.full(shape=DIMENSNIONS, fill_value=0.1).tolist()\n",
    "\n",
    "print(acl_query(group_ids, doctype_ids, workspace, 10, vector))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step #5 - Test delete by doc_id\n",
    "\n",
    "1. Pick an id from the previous output\n",
    "1. Get the base 0 doc id (this id has the chunk count in a meta-data field)\n",
    "1. Fetch the chunk count for this base doc id from pinecone\n",
    "1. Use the chunk count to create a list of chunk ids to delete\n",
    "1. Delete the chunk ids from the index(way more efficient vs delete by metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 'document-7-16'\n",
    "delete_ids = []\n",
    "\n",
    "def get_doc_id(id):\n",
    "    parts = id.rsplit('-', 1)\n",
    "    doc_id = parts[0] if len(parts) > 1 else doc_id   \n",
    "    return f\"{doc_id}-0\"\n",
    "\n",
    "def set_delete_ids(doc_id, chunk_count):\n",
    "    for i in range(int(chunk_count)):\n",
    "        delete_ids.append(f\"{doc_id[:-2]}-{i}\")\n",
    "\n",
    "chunk_count = index.fetch(ids=[get_doc_id(id)], namespace=workspace).vectors[get_doc_id(id)][\"metadata\"][\"chunk_count\"]\n",
    "set_delete_ids(get_doc_id(id), chunk_count)\n",
    "print(f\"IDs that will be deleted: {delete_ids}\")\n",
    "index.delete(ids=delete_ids, namespace=workspace)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 - Validate that the document chunks have been deleted from the index\n",
    "\n",
    "There should be 50 less total vectors. 5000 to 4950."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert index.fetch(ids=[get_doc_id(id)], namespace=workspace).vectors == {}\n",
    "\n",
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7 [OPTIONAL] - Delete all vectors from the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.delete(delete_all=True, namespace=\"1\")\n",
    "index.delete(delete_all=True, namespace=\"2\")\n",
    "index.delete(delete_all=True, namespace=\"3\")\n",
    "\n",
    "index.describe_index_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
